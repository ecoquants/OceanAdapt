---
title: "Troubleshooting Ocean Adapt"
output: html_notebook
---

As of 2018-09-17 there are 2 problems with the ocean adapt website:

#1. There are corrupt lines in the SEUS files:
*Problem: SEUS Not generating properly:*

*Some corruption at the bottom of the SEUS_X.csv files; seems like metadata. This might be interfering with this region being generated.*
*Cleaning these lines up should allow the script to run properly or at least let us troubleshoot the script further. Last year's SEUS data seems to work properly with the current script.*

*SEUS_HAUL.csv*
*Lines: 17208 to 17225*
*SEUS_CATCH.csv*
*Lines: 391317 to 391334*

```{r}
# get the raw csv's - UNZIP THE RAW PACKAGE OF FILES MANUALLY!!
# readr:read_csv gave error about col_align and namespace:crayon
haul <- read.csv("data_raw/seus/2018-08-11/seus_haul.csv")
catch <- read.csv("data_raw/seus/2018-08-11/seus_catch.csv")
```
## 1.1 There are only 17218 rows in the raw file, though 17205-17218 do look different than the rest of the rows.  Check the updated file - don't have a copy of the corrupt file, proceeding with the raw data
```{r}
library(dplyr)
haul <- haul %>% 
  slice(1:17204)

# check that there aren't any weird rows hidden in the middle of the file
test <- haul %>% 
  group_by(PROJECTNAME) %>% 
  summarise(count = n())

# all values in the first column are now "=Coastal Survey"

# repeat for catch
test <- catch %>% 
  group_by(PROJECTNAME) %>% 
  summarise(count = n())
# there are 9 blank rows, 1 each of some categories that were also removed from the haul data, and the remainder are "=Coastal Survey"
catch <- catch %>% 
  filter(PROJECTNAME == "=Coastal Survey")

write.csv(haul, file = "data_raw/seus/2018-08-11/seus_haul.csv", row.names = F)
write.csv(catch, file = "data_raw/seus/2018-08-11/seus_catch.csv", row.names = F)
```


2. Warning Notice [#2]: regional data files being deleted after script is ran

The original script that Ryan created last year had all of the CSV files in a ZIP file in the `./data_updates/` folder.  The complete_R_script would extract the CSVs from the ZIP file, perform all its processing, and then delete the CSVs that it had extracted as a clean up process.  This allowed the script to be run any number of times since it would just re-extract the CSVs from the ZIP file.  The current file structure that Michelle is submitting no longer has the CSVs in a separate zip file but just has them in the folder already extracted.  The issue is that the cleanup process still exists in the complete_R_script and now when it finishes, it deletes all the CSV files in that folder, making it so the data is no longer there and the script can't be run again unless the user downloads the entire package again from the website.  We recommend either re-implementing the zip file of CSVs in the ./data_updates folder or removing the code from the complete_r_script that deletes the CSVs when it is done processing.

```{r}
# need to reimport all of the data because the faulty data/update was deleted.



```
